{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d162b6bb-04be-41c3-94af-44ab59e6500d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00c0043-c7ca-4492-ad2f-cced9d6fae30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb2e3631-6704-4cac-8a49-88743235ff8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values in 'company_name' column: 8971\n",
      "Number of unique values in 'status_label' column: 2\n",
      "Number of unique values in 'Division' column: 10\n",
      "Number of unique values in 'MajorGroup' column: 73\n",
      "Number of unique values in 'last_year' column: 20\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the data set\n",
    "data = pd.read_csv(\"new_df_selected2_last2years_adjusted.csv\")\n",
    "\n",
    "unique_company_names = data['company_name'].nunique()\n",
    "unique_status_labels = data['status_label'].nunique()\n",
    "unique_divisions = data['Division'].nunique()\n",
    "unique_majorgroup = data['MajorGroup'].nunique()\n",
    "unique_last_year = data['last_year'].nunique()\n",
    "\n",
    "print(\"Number of unique values in 'company_name' column:\", unique_company_names)\n",
    "print(\"Number of unique values in 'status_label' column:\", unique_status_labels)\n",
    "print(\"Number of unique values in 'Division' column:\", unique_divisions)\n",
    "print(\"Number of unique values in 'MajorGroup' column:\", unique_majorgroup)\n",
    "print(\"Number of unique values in 'last_year' column:\", unique_last_year)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263fa8e8-cf02-4f9e-a23d-74af29abb22c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0552208d-295c-4128-90b8-ed6e4d3a1494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8971 entries, 0 to 8970\n",
      "Data columns (total 81 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   company_name       8971 non-null   object \n",
      " 1   status_label       8971 non-null   object \n",
      " 2   Division           8971 non-null   object \n",
      " 3   MajorGroup         8971 non-null   int64  \n",
      " 4   last_year          8971 non-null   float64\n",
      " 5   X1_last1year       8971 non-null   float64\n",
      " 6   X1_last2year       8078 non-null   float64\n",
      " 7   X2_last1year       8971 non-null   float64\n",
      " 8   X2_last2year       8078 non-null   float64\n",
      " 9   X3_last1year       8971 non-null   float64\n",
      " 10  X3_last2year       8078 non-null   float64\n",
      " 11  X4_last1year       8971 non-null   float64\n",
      " 12  X4_last2year       8078 non-null   float64\n",
      " 13  X5_last1year       8971 non-null   float64\n",
      " 14  X5_last2year       8078 non-null   float64\n",
      " 15  X6_last1year       8971 non-null   float64\n",
      " 16  X6_last2year       8078 non-null   float64\n",
      " 17  X7_last1year       8971 non-null   float64\n",
      " 18  X7_last2year       8078 non-null   float64\n",
      " 19  X8_last1year       8971 non-null   float64\n",
      " 20  X8_last2year       8078 non-null   float64\n",
      " 21  X9_last1year       8971 non-null   float64\n",
      " 22  X9_last2year       8078 non-null   float64\n",
      " 23  X10_last1year      8971 non-null   float64\n",
      " 24  X10_last2year      8078 non-null   float64\n",
      " 25  X11_last1year      8971 non-null   float64\n",
      " 26  X11_last2year      8078 non-null   float64\n",
      " 27  X12_last1year      8971 non-null   float64\n",
      " 28  X12_last2year      8078 non-null   float64\n",
      " 29  X13_last1year      8971 non-null   float64\n",
      " 30  X13_last2year      8078 non-null   float64\n",
      " 31  X14_last1year      8971 non-null   float64\n",
      " 32  X14_last2year      8078 non-null   float64\n",
      " 33  X15_last1year      8971 non-null   float64\n",
      " 34  X15_last2year      8078 non-null   float64\n",
      " 35  X16_last1year      8971 non-null   float64\n",
      " 36  X16_last2year      8078 non-null   float64\n",
      " 37  X17_last1year      8971 non-null   float64\n",
      " 38  X17_last2year      8078 non-null   float64\n",
      " 39  X18_last1year      8971 non-null   float64\n",
      " 40  X18_last2year      8078 non-null   float64\n",
      " 41  X1_last1year_ycr   8078 non-null   float64\n",
      " 42  X1_last2year_ycr   7101 non-null   float64\n",
      " 43  X2_last1year_ycr   8078 non-null   float64\n",
      " 44  X2_last2year_ycr   7101 non-null   float64\n",
      " 45  X3_last1year_ycr   8078 non-null   float64\n",
      " 46  X3_last2year_ycr   7101 non-null   float64\n",
      " 47  X4_last1year_ycr   8078 non-null   float64\n",
      " 48  X4_last2year_ycr   7101 non-null   float64\n",
      " 49  X5_last1year_ycr   8078 non-null   float64\n",
      " 50  X5_last2year_ycr   7101 non-null   float64\n",
      " 51  X6_last1year_ycr   8078 non-null   float64\n",
      " 52  X6_last2year_ycr   7101 non-null   float64\n",
      " 53  X7_last1year_ycr   8078 non-null   float64\n",
      " 54  X7_last2year_ycr   7101 non-null   float64\n",
      " 55  X8_last1year_ycr   8078 non-null   float64\n",
      " 56  X8_last2year_ycr   7101 non-null   float64\n",
      " 57  X9_last1year_ycr   8078 non-null   float64\n",
      " 58  X9_last2year_ycr   7101 non-null   float64\n",
      " 59  X10_last1year_ycr  8078 non-null   float64\n",
      " 60  X10_last2year_ycr  7101 non-null   float64\n",
      " 61  X11_last1year_ycr  8078 non-null   float64\n",
      " 62  X11_last2year_ycr  7101 non-null   float64\n",
      " 63  X12_last1year_ycr  8078 non-null   float64\n",
      " 64  X12_last2year_ycr  7101 non-null   float64\n",
      " 65  X13_last1year_ycr  8078 non-null   float64\n",
      " 66  X13_last2year_ycr  7101 non-null   float64\n",
      " 67  X14_last1year_ycr  8078 non-null   float64\n",
      " 68  X14_last2year_ycr  7101 non-null   float64\n",
      " 69  X15_last1year_ycr  8078 non-null   float64\n",
      " 70  X15_last2year_ycr  7101 non-null   float64\n",
      " 71  X16_last1year_ycr  8078 non-null   float64\n",
      " 72  X16_last2year_ycr  7101 non-null   float64\n",
      " 73  X17_last1year_ycr  8078 non-null   float64\n",
      " 74  X17_last2year_ycr  7101 non-null   float64\n",
      " 75  X18_last1year_ycr  8078 non-null   float64\n",
      " 76  X18_last2year_ycr  7101 non-null   float64\n",
      " 77  nyse_last1year     8971 non-null   float64\n",
      " 78  nyse_last2year     8496 non-null   float64\n",
      " 79  nasdaq_last1year   8971 non-null   float64\n",
      " 80  nasdaq_last2year   8496 non-null   float64\n",
      "dtypes: float64(77), int64(1), object(3)\n",
      "memory usage: 5.5+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc4a5663-66c6-4f87-84bb-ff9ab875eba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  company_name status_label Division  MajorGroup  last_year  X1_last1year  \\\n",
      "0          C_1        alive        D          37     2017.0         942.7   \n",
      "1          C_2        alive        D          36     2010.0        1107.7   \n",
      "2          C_3        alive        D          38     2008.0       12686.0   \n",
      "3          C_4        alive        D          28     2007.0      581502.0   \n",
      "4          C_5        alive        D          35     1999.0       28957.0   \n",
      "\n",
      "   X1_last2year  X2_last1year  X2_last2year  X3_last1year  ...  \\\n",
      "0         888.5       1524.70        1504.1       1413.20  ...   \n",
      "1         900.2       1474.50        1343.6        677.20  ...   \n",
      "2       13454.0      21401.00       27171.0      19334.00  ...   \n",
      "3      353541.0    1288165.00      927239.0        267.81  ...   \n",
      "4           NaN         42.21           NaN      79567.00  ...   \n",
      "\n",
      "   X18_last1year_ycr  X18_last2year_ycr  nyse_last1year  nyse_last2year  \\\n",
      "0           0.001482           0.061414    11912.848307    10451.377523   \n",
      "1           0.135088          -0.289221     7166.229940     6100.795776   \n",
      "2           0.269244           0.227659     8001.502441     9685.001790   \n",
      "3          -0.998676           0.318548     9685.001790     8434.441610   \n",
      "4                NaN                NaN     6549.827474             NaN   \n",
      "\n",
      "   nasdaq_last1year  nasdaq_last2year  company_name_encoded  Division_encoded  \\\n",
      "0       6293.024211       5015.926717                     0                 3   \n",
      "1       2333.908345       1856.529999                  1111                 3   \n",
      "2       2148.948334       2587.587504                  2222                 3   \n",
      "3       2587.587504       2278.996664                  3333                 3   \n",
      "4       2787.559998               NaN                  4444                 3   \n",
      "\n",
      "   MajorGroup_encoded  status_label_encoded  \n",
      "0                  29                     0  \n",
      "1                  28                     0  \n",
      "2                  30                     0  \n",
      "3                  20                     0  \n",
      "4                  27                     0  \n",
      "\n",
      "[5 rows x 85 columns]\n"
     ]
    }
   ],
   "source": [
    "# Encoding non-numeric columns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize LabelEncoder object\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "\n",
    "# Label-encode the company_name column\n",
    "data['company_name_encoded'] = label_encoder.fit_transform(data['company_name'])\n",
    "\n",
    "\n",
    "# Label-encode the Division column\n",
    "data['Division_encoded'] = label_encoder.fit_transform(data['Division'])\n",
    "\n",
    "# Label-encode MajorGroup columns\n",
    "data['MajorGroup_encoded'] = label_encoder.fit_transform(data['MajorGroup'])\n",
    "#When using label encoding for feature encoding, the sequential relationship between categories will not be introduced and will not have an impact on prediction.\n",
    "\n",
    "# Encode the label of the status_label column\n",
    "data['status_label_encoded'] = label_encoder.fit_transform(data['status_label'])\n",
    "#With only two categories, it may be simpler and more appropriate to use label encoding as it maps the categories to 0 and 1, suitable for use in tree-based models. \n",
    "#If use one-hot encoding, a new column will be generated\n",
    "\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1f286d3-0047-4da4-90dc-dd63a2e84a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8971 entries, 0 to 8970\n",
      "Data columns (total 85 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   company_name          8971 non-null   object \n",
      " 1   status_label          8971 non-null   object \n",
      " 2   Division              8971 non-null   object \n",
      " 3   MajorGroup            8971 non-null   int64  \n",
      " 4   last_year             8971 non-null   float64\n",
      " 5   X1_last1year          8971 non-null   float64\n",
      " 6   X1_last2year          8078 non-null   float64\n",
      " 7   X2_last1year          8971 non-null   float64\n",
      " 8   X2_last2year          8078 non-null   float64\n",
      " 9   X3_last1year          8971 non-null   float64\n",
      " 10  X3_last2year          8078 non-null   float64\n",
      " 11  X4_last1year          8971 non-null   float64\n",
      " 12  X4_last2year          8078 non-null   float64\n",
      " 13  X5_last1year          8971 non-null   float64\n",
      " 14  X5_last2year          8078 non-null   float64\n",
      " 15  X6_last1year          8971 non-null   float64\n",
      " 16  X6_last2year          8078 non-null   float64\n",
      " 17  X7_last1year          8971 non-null   float64\n",
      " 18  X7_last2year          8078 non-null   float64\n",
      " 19  X8_last1year          8971 non-null   float64\n",
      " 20  X8_last2year          8078 non-null   float64\n",
      " 21  X9_last1year          8971 non-null   float64\n",
      " 22  X9_last2year          8078 non-null   float64\n",
      " 23  X10_last1year         8971 non-null   float64\n",
      " 24  X10_last2year         8078 non-null   float64\n",
      " 25  X11_last1year         8971 non-null   float64\n",
      " 26  X11_last2year         8078 non-null   float64\n",
      " 27  X12_last1year         8971 non-null   float64\n",
      " 28  X12_last2year         8078 non-null   float64\n",
      " 29  X13_last1year         8971 non-null   float64\n",
      " 30  X13_last2year         8078 non-null   float64\n",
      " 31  X14_last1year         8971 non-null   float64\n",
      " 32  X14_last2year         8078 non-null   float64\n",
      " 33  X15_last1year         8971 non-null   float64\n",
      " 34  X15_last2year         8078 non-null   float64\n",
      " 35  X16_last1year         8971 non-null   float64\n",
      " 36  X16_last2year         8078 non-null   float64\n",
      " 37  X17_last1year         8971 non-null   float64\n",
      " 38  X17_last2year         8078 non-null   float64\n",
      " 39  X18_last1year         8971 non-null   float64\n",
      " 40  X18_last2year         8078 non-null   float64\n",
      " 41  X1_last1year_ycr      8078 non-null   float64\n",
      " 42  X1_last2year_ycr      7101 non-null   float64\n",
      " 43  X2_last1year_ycr      8078 non-null   float64\n",
      " 44  X2_last2year_ycr      7101 non-null   float64\n",
      " 45  X3_last1year_ycr      8078 non-null   float64\n",
      " 46  X3_last2year_ycr      7101 non-null   float64\n",
      " 47  X4_last1year_ycr      8078 non-null   float64\n",
      " 48  X4_last2year_ycr      7101 non-null   float64\n",
      " 49  X5_last1year_ycr      8078 non-null   float64\n",
      " 50  X5_last2year_ycr      7101 non-null   float64\n",
      " 51  X6_last1year_ycr      8078 non-null   float64\n",
      " 52  X6_last2year_ycr      7101 non-null   float64\n",
      " 53  X7_last1year_ycr      8078 non-null   float64\n",
      " 54  X7_last2year_ycr      7101 non-null   float64\n",
      " 55  X8_last1year_ycr      8078 non-null   float64\n",
      " 56  X8_last2year_ycr      7101 non-null   float64\n",
      " 57  X9_last1year_ycr      8078 non-null   float64\n",
      " 58  X9_last2year_ycr      7101 non-null   float64\n",
      " 59  X10_last1year_ycr     8078 non-null   float64\n",
      " 60  X10_last2year_ycr     7101 non-null   float64\n",
      " 61  X11_last1year_ycr     8078 non-null   float64\n",
      " 62  X11_last2year_ycr     7101 non-null   float64\n",
      " 63  X12_last1year_ycr     8078 non-null   float64\n",
      " 64  X12_last2year_ycr     7101 non-null   float64\n",
      " 65  X13_last1year_ycr     8078 non-null   float64\n",
      " 66  X13_last2year_ycr     7101 non-null   float64\n",
      " 67  X14_last1year_ycr     8078 non-null   float64\n",
      " 68  X14_last2year_ycr     7101 non-null   float64\n",
      " 69  X15_last1year_ycr     8078 non-null   float64\n",
      " 70  X15_last2year_ycr     7101 non-null   float64\n",
      " 71  X16_last1year_ycr     8078 non-null   float64\n",
      " 72  X16_last2year_ycr     7101 non-null   float64\n",
      " 73  X17_last1year_ycr     8078 non-null   float64\n",
      " 74  X17_last2year_ycr     7101 non-null   float64\n",
      " 75  X18_last1year_ycr     8078 non-null   float64\n",
      " 76  X18_last2year_ycr     7101 non-null   float64\n",
      " 77  nyse_last1year        8971 non-null   float64\n",
      " 78  nyse_last2year        8496 non-null   float64\n",
      " 79  nasdaq_last1year      8971 non-null   float64\n",
      " 80  nasdaq_last2year      8496 non-null   float64\n",
      " 81  company_name_encoded  8971 non-null   int32  \n",
      " 82  Division_encoded      8971 non-null   int32  \n",
      " 83  MajorGroup_encoded    8971 non-null   int64  \n",
      " 84  status_label_encoded  8971 non-null   int32  \n",
      "dtypes: float64(77), int32(3), int64(2), object(3)\n",
      "memory usage: 5.7+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "374fe89d-42d2-472f-8724-dcc6f06dff33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8971, 85)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11b45588-522a-4d3c-8509-9749f7d3c4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values in 'company_name_encoded' column: 8971\n",
      "Number of unique values in 'status_label_encoded' column: 2\n",
      "Number of unique values in 'Division_encoded' column: 10\n",
      "Number of unique values in 'MajorGroup_encoded' column: 73\n"
     ]
    }
   ],
   "source": [
    "unique_company_names = data['company_name_encoded'].nunique()\n",
    "unique_status_labels = data['status_label_encoded'].nunique()\n",
    "unique_divisions = data['Division_encoded'].nunique()\n",
    "unique_majorgroup = data['MajorGroup_encoded'].nunique()\n",
    "\n",
    "print(\"Number of unique values in 'company_name_encoded' column:\", unique_company_names)\n",
    "print(\"Number of unique values in 'status_label_encoded' column:\", unique_status_labels)\n",
    "print(\"Number of unique values in 'Division_encoded' column:\", unique_divisions)\n",
    "print(\"Number of unique values in 'MajorGroup_encoded' column:\", unique_majorgroup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1abf0ae-6966-43bc-a652-7523ee634cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 'Division_encoded' column: [3 4 2 8 5 6 1 0 7 9]\n"
     ]
    }
   ],
   "source": [
    "unique_divisions = data['Division_encoded'].unique()\n",
    "print(\"Unique values in 'Division_encoded' column:\", unique_divisions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "381247f3-2cdf-4adf-b51a-e16ec84e1347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with missing values: 1870\n"
     ]
    }
   ],
   "source": [
    "missing_rows_count = data.isnull().any(axis=1).sum()\n",
    "print(\"Number of rows with missing values:\", missing_rows_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b98d5d1a-22e0-4ee6-8fc9-f156a2a97b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7101, 79)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete rows with missing values\n",
    "data_cleaned = data.dropna()\n",
    "# Delete non-numeric columns that are not encoded\n",
    "# Delete specified column\n",
    "data_cleaned = data_cleaned.drop(['company_name', 'status_label', 'Division', 'MajorGroup', 'last_year', 'company_name_encoded'], axis=1)\n",
    "\n",
    "data_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22f1ba13-272c-4957-96db-dd37c4602510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of       X1_last1year  X1_last2year  X2_last1year  X2_last2year  X3_last1year  \\\n",
       "0            942.7         888.5       1524.70       1504.10       1413.20   \n",
       "1           1107.7         900.2       1474.50       1343.60        677.20   \n",
       "2          12686.0       13454.0      21401.00      27171.00      19334.00   \n",
       "3         581502.0      353541.0    1288165.00     927239.00        267.81   \n",
       "5           6838.0        6642.0      25088.00      25438.00      18138.00   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "8966       10566.0       11738.0      28278.00      26206.00      31288.00   \n",
       "8967        3369.0        9049.0       3466.00       9198.00        208.00   \n",
       "8968        2482.2        2340.6       9401.50      10252.40        966.70   \n",
       "8969         931.6        1032.7       2810.20       2542.00       1475.90   \n",
       "8970       82589.0      135207.0       1625.37       1736.11      68817.00   \n",
       "\n",
       "      X3_last2year  X4_last1year  X4_last2year  X5_last1year  X5_last2year  \\\n",
       "0         1422.700         177.2         155.3        40.500        71.000   \n",
       "1          600.500         650.8         651.0        61.500        66.400   \n",
       "2           17.589          23.0        5822.0      1686.000         0.500   \n",
       "3       229115.000         300.0           0.0     46338.000        42.873   \n",
       "5        16935.000        9253.0       10583.0       995.000      1007.000   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "8966     26885.000        8497.0        8549.0      1200.000      1133.000   \n",
       "8967      1077.000           0.0           0.0        57.000         0.190   \n",
       "8968      1833.200        5350.7        5440.6       156.600       354.200   \n",
       "8969      1269.600        1409.5        1442.3        82.500        62.100   \n",
       "8970     66527.000      632122.0      694035.0        65.201        65.330   \n",
       "\n",
       "      ...  X17_last2year_ycr  X18_last1year_ycr  X18_last2year_ycr  \\\n",
       "0     ...           0.063154           0.001482           0.061414   \n",
       "1     ...          -0.315641           0.135088          -0.289221   \n",
       "2     ...          -0.998851           0.269244           0.227659   \n",
       "3     ...           0.181011          -0.998676           0.318548   \n",
       "5     ...          -0.161954           0.061664          -0.154842   \n",
       "...   ...                ...                ...                ...   \n",
       "8966  ...           0.181647           0.165494           0.207771   \n",
       "8967  ...         413.133739          -0.999197          -0.618622   \n",
       "8968  ...           0.052985          -0.486944           0.054327   \n",
       "8969  ...           0.076050           0.147645           0.053146   \n",
       "8970  ...          -0.046945           0.005803           0.020990   \n",
       "\n",
       "      nyse_last1year  nyse_last2year  nasdaq_last1year  nasdaq_last2year  \\\n",
       "0       11912.848307    10451.377523       6293.024211       5015.926717   \n",
       "1        7166.229940     6100.795776       2333.908345       1856.529999   \n",
       "2        8001.502441     9685.001790       2148.948334       2587.587504   \n",
       "3        9685.001790     8434.441610       2587.587504       2278.996664   \n",
       "5        7166.229940     6100.795776       2333.908345       1856.529999   \n",
       "...              ...             ...               ...               ...   \n",
       "8966    12593.500651    11912.848307       7405.502482       6293.024211   \n",
       "8967    11912.848307    10451.377523       6293.024211       5015.926717   \n",
       "8968    12593.500651    11912.848307       7405.502482       6293.024211   \n",
       "8969    12593.500651    11912.848307       7405.502482       6293.024211   \n",
       "8970    12593.500651    11912.848307       7405.502482       6293.024211   \n",
       "\n",
       "      Division_encoded  MajorGroup_encoded  status_label_encoded  \n",
       "0                    3                  29                     0  \n",
       "1                    3                  28                     0  \n",
       "2                    3                  30                     0  \n",
       "3                    3                  20                     0  \n",
       "5                    4                  36                     1  \n",
       "...                ...                 ...                   ...  \n",
       "8966                 3                  20                     0  \n",
       "8967                 3                  30                     0  \n",
       "8968                 3                  20                     0  \n",
       "8969                 8                  60                     0  \n",
       "8970                 4                  35                     0  \n",
       "\n",
       "[7101 rows x 79 columns]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaned.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89dd5f1e-dc82-4769-b937-5bb68e3d9f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    6537\n",
      "1     564\n",
      "Name: status_label_encoded, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "status_counts = data_cleaned['status_label_encoded'].value_counts()\n",
    "print(status_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a330463-b2f8-4aa1-85a6-f2ad40621a69",
   "metadata": {},
   "source": [
    "### 1. RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f159c74-a2f6-46b8-b156-d9db13dd3a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the impact of imbalanced datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317d5616-60a7-4288-b8a2-6630bd5fbe7d",
   "metadata": {},
   "source": [
    "#### 1.1 imbalance dataset + CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90fff82c-02bc-4660-86fb-87d2b8ed60f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.926056338028169\n",
      "Cross-validation Scores: [0.92517606 0.92341549 0.92165493 0.92693662 0.92253521]\n",
      "Mean CV Accuracy: 0.923943661971831\n",
      "Test Accuracy: 0.9204785362420831\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split the dataset into features and labels\n",
    "X = data_cleaned.drop('status_label_encoded', axis=1)\n",
    "y = data_cleaned['status_label_encoded']\n",
    "\n",
    "# Split the data set into training set, validation set and test set\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)  # 80% training, 20% validation\n",
    "\n",
    "# Define a random forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# Train the model on the training set\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "val_predictions = rf_classifier.predict(X_val)\n",
    "# Calculate accuracy on validation set\n",
    "val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n",
    "\n",
    "# Perform cross-validation on the training set\n",
    "cv_scores = cross_val_score(rf_classifier, X_train_val, y_train_val, cv=5)\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation Scores:\", cv_scores)\n",
    "print(\"Mean CV Accuracy:\", cv_scores.mean())\n",
    "# Retrain the model on the entire training set\n",
    "\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = rf_classifier.predict(X_test)\n",
    "# Calculate accuracy on test set\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef9d866c-bd00-40bf-9b07-b63d8c2b2bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9204785362420831\n",
      "Precision: 0.6666666666666666\n",
      "Recall: 0.017543859649122806\n",
      "F1 Score: 0.03418803418803419\n",
      "Micro F1 Score: 0.9204785362420831\n",
      "Macro F1 Score: 0.4963600721398886\n",
      "ROC AUC: 0.811735727996349\n",
      "Confusion Matrix:\n",
      "[[1306    1]\n",
      " [ 112    2]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, precision_recall_curve, roc_curve, confusion_matrix\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Calculation precision\n",
    "precision = precision_score(y_test, test_predictions)\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_test, test_predictions)\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_test, test_predictions)\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "# Calculate micro F1 score\n",
    "micro_f1 = f1_score(y_test, test_predictions, average='micro')\n",
    "print(\"Micro F1 Score:\", micro_f1)\n",
    "\n",
    "# Calculate macro F1 score\n",
    "macro_f1 = f1_score(y_test, test_predictions, average='macro')\n",
    "print(\"Macro F1 Score:\", macro_f1)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "y_prob = rf_classifier.predict_proba(X_test)[:, 1]\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec2ca22-1117-4c1e-bebc-db9b174d16be",
   "metadata": {},
   "source": [
    "#### 1.2 SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fbcd72-d620-4567-92ec-670b1b3319ac",
   "metadata": {},
   "source": [
    "##### 1.2.1 SMOTE + cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "728e436f-44d3-46fd-9903-b2c9f1f19de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    3919\n",
      "1    3919\n",
      "Name: status_label_encoded, dtype: int64\n",
      "Validation Accuracy: 0.9169014084507042\n",
      "Cross-validation Scores: [0.90226782 0.97678186 0.98542117 0.98487304 0.94111291]\n",
      "Mean CV Accuracy: 0.9580913591742481\n",
      "Test Accuracy: 0.9183673469387755\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into features and labels\n",
    "X = data_cleaned.drop('status_label_encoded', axis=1)\n",
    "y = data_cleaned['status_label_encoded']\n",
    "\n",
    "# Split the data into training + validation and testing sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Further split the training + validation set into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)\n",
    "\n",
    "# Apply SMOTE only on the training set\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Print the number of samples after oversampling\n",
    "print(pd.Series(y_train_resampled).value_counts())\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define a random forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# Train the model on the training set\n",
    "rf_classifier.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "val_predictions = rf_classifier.predict(X_val)\n",
    "# Calculate accuracy on validation set\n",
    "val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n",
    "\n",
    "# Combine resampled training set and validation set\n",
    "X_train_val_reshape = pd.concat([pd.DataFrame(X_train_resampled), pd.DataFrame(X_val)], axis=0)\n",
    "y_train_val_reshape = pd.concat([pd.Series(y_train_resampled), pd.Series(y_val)], axis=0)\n",
    "\n",
    "\n",
    "# Perform cross-validation on the train_val set\n",
    "cv_scores = cross_val_score(rf_classifier, X_train_val_reshape, y_train_val_reshape, cv=5)\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation Scores:\", cv_scores)\n",
    "print(\"Mean CV Accuracy:\", cv_scores.mean())\n",
    "# Retrain the model on the train_val set\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = rf_classifier.predict(X_test)\n",
    "# Calculate accuracy on test set\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e483ae23-d2a2-4585-bf4b-a4a62e6d5fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9183673469387755\n",
      "Precision: 0.48333333333333334\n",
      "Recall: 0.2543859649122807\n",
      "F1 Score: 0.33333333333333337\n",
      "Micro F1 Score: 0.9183673469387755\n",
      "Macro F1 Score: 0.644927536231884\n",
      "ROC AUC: 0.8333702465804911\n",
      "Confusion Matrix:\n",
      "[[1276   31]\n",
      " [  85   29]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, precision_recall_curve, roc_curve, confusion_matrix\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test, test_predictions)\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_test, test_predictions)\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_test, test_predictions)\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "# Calculate micro F1 score\n",
    "micro_f1 = f1_score(y_test, test_predictions, average='micro')\n",
    "print(\"Micro F1 Score:\", micro_f1)\n",
    "\n",
    "# Calculate macro F1 score\n",
    "macro_f1 = f1_score(y_test, test_predictions, average='macro')\n",
    "print(\"Macro F1 Score:\", macro_f1)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "y_prob = rf_classifier.predict_proba(X_test)[:, 1]\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f701d5f-c473-4ad0-91f1-33a87ee76802",
   "metadata": {},
   "source": [
    "##### 1.2.2 SMOTE + CV + FS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e8954c-8861-4ab4-9195-8e51739822e4",
   "metadata": {},
   "source": [
    "##### 1.2.3 SMOTE + GRID + FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e005918e-9f5a-4fbe-a1c2-57b3de3bf4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features:\n",
      "Index(['X1_last2year', 'X3_last1year', 'X4_last1year', 'X4_last2year',\n",
      "       'X5_last2year', 'X6_last1year', 'X9_last1year', 'X9_last2year',\n",
      "       'X10_last1year', 'X10_last2year', 'X11_last1year', 'X11_last2year',\n",
      "       'X12_last1year', 'X12_last2year', 'X15_last1year', 'X16_last1year',\n",
      "       'X16_last2year', 'X18_last2year', 'X1_last1year_ycr',\n",
      "       'X1_last2year_ycr', 'X4_last1year_ycr', 'X4_last2year_ycr',\n",
      "       'X5_last2year_ycr', 'X9_last1year_ycr', 'X9_last2year_ycr',\n",
      "       'X10_last1year_ycr', 'X10_last2year_ycr', 'X13_last1year_ycr',\n",
      "       'X13_last2year_ycr', 'X14_last2year_ycr', 'X15_last1year_ycr',\n",
      "       'X16_last2year_ycr', 'X18_last2year_ycr', 'nyse_last1year',\n",
      "       'nyse_last2year', 'nasdaq_last1year', 'nasdaq_last2year',\n",
      "       'Division_encoded', 'MajorGroup_encoded'],\n",
      "      dtype='object')\n",
      "Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Best Score: 0.9566268737871664\n",
      "Validation Accuracy with Best Model: 0.9133802816901408\n",
      "Test Accuracy with Best Model: 0.9106263194933145\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Split the dataset into features and labels\n",
    "X = data_cleaned.drop('status_label_encoded', axis=1)\n",
    "y = data_cleaned['status_label_encoded']\n",
    "\n",
    "# Split the data into training + validation and testing sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Further split the training + validation set into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)\n",
    "\n",
    "# Apply SMOTE only on the training set\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define a random forest classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Use SelectFromModel for feature selection\n",
    "selector = SelectFromModel(estimator=rf_classifier, threshold='median')  #better precision, worse recall&f1, compare with mean\n",
    "X_train_resampled_selected = selector.fit_transform(X_train_resampled, y_train_resampled)\n",
    "selected_features = X_train_resampled.columns[selector.get_support()]   #Select a list of column names for the characteristics.\n",
    "print(\"Selected Features:\")\n",
    "print(selected_features)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train_resampled_selected, y_train_resampled)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# Get the best model\n",
    "best_rf_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the validation set with selected features\n",
    "X_val_selected = selector.transform(X_val)\n",
    "val_predictions = best_rf_classifier.predict(X_val_selected)\n",
    "# Calculate accuracy on validation set\n",
    "val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "print(\"Validation Accuracy with Best Model:\", val_accuracy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions on the test set with the best model\n",
    "X_test_selected = selector.transform(X_test)\n",
    "test_predictions = best_rf_classifier.predict(X_test_selected)\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(\"Test Accuracy with Best Model:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0e87ddb-5dd8-44d4-a086-1c8eff459c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9106263194933145\n",
      "Precision: 0.4084507042253521\n",
      "Recall: 0.2543859649122807\n",
      "F1 Score: 0.31351351351351353\n",
      "Micro F1 Score: 0.9106263194933145\n",
      "Macro F1 Score: 0.6328576223946942\n",
      "ROC AUC: 0.840454234285024\n",
      "Confusion Matrix:\n",
      "[[1265   42]\n",
      " [  85   29]]\n"
     ]
    }
   ],
   "source": [
    "#The evaluation metrics on the test set are calculated directly without threshold selection and re-prediction.\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, precision_recall_curve, roc_curve, confusion_matrix\n",
    "\n",
    "#Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test, test_predictions)\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_test, test_predictions)\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_test, test_predictions)\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "# Calculate micro F1 score\n",
    "micro_f1 = f1_score(y_test, test_predictions, average='micro')\n",
    "print(\"Micro F1 Score:\", micro_f1)\n",
    "\n",
    "# Calculate macro F1 score\n",
    "macro_f1 = f1_score(y_test, test_predictions, average='macro')\n",
    "print(\"Macro F1 Score:\", macro_f1)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "y_prob = best_rf_classifier.predict_proba(X_test_selected)[:, 1]\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc679609-6dab-41cc-91a5-9dd2a1dde22c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1.3 processing imbalanced dataset, Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27b3b807-7807-42db-bdcf-0d82c53d4a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# undersampling +grid +fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a76a68fa-dc0d-4cb3-8d15-dc35763e04b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features:\n",
      "Index(['X2_last1year', 'X3_last1year', 'X4_last2year', 'X5_last1year',\n",
      "       'X5_last2year', 'X6_last1year', 'X6_last2year', 'X7_last1year',\n",
      "       'X11_last1year', 'X11_last2year', 'X12_last1year', 'X12_last2year',\n",
      "       'X15_last1year', 'X15_last2year', 'X16_last1year', 'X16_last2year',\n",
      "       'X18_last2year', 'X1_last1year_ycr', 'X2_last1year_ycr',\n",
      "       'X2_last2year_ycr', 'X4_last1year_ycr', 'X4_last2year_ycr',\n",
      "       'X7_last2year_ycr', 'X8_last1year_ycr', 'X10_last1year_ycr',\n",
      "       'X11_last1year_ycr', 'X11_last2year_ycr', 'X13_last2year_ycr',\n",
      "       'X15_last1year_ycr', 'X15_last2year_ycr', 'X16_last1year_ycr',\n",
      "       'X16_last2year_ycr', 'X18_last1year_ycr', 'X18_last2year_ycr',\n",
      "       'nyse_last1year', 'nyse_last2year', 'nasdaq_last1year',\n",
      "       'nasdaq_last2year', 'MajorGroup_encoded'],\n",
      "      dtype='object')\n",
      "Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "Best Score: 0.7786389008158007\n",
      "Validation Accuracy with Best Model: 0.7232394366197183\n",
      "Test Accuracy with Best Model: 0.7325826882477129\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Split the dataset into features and labels\n",
    "X = data_cleaned.drop('status_label_encoded', axis=1)\n",
    "y = data_cleaned['status_label_encoded']\n",
    "\n",
    "# Split the data into training + validation and testing sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Further split the training + validation set into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)\n",
    "\n",
    "# Define an undersampler\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = undersampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define a random forest classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Use SelectFromModel for feature selection\n",
    "selector = SelectFromModel(estimator=rf_classifier, threshold='median')  #better precision, worse recall&f1, compare with mean\n",
    "X_train_resampled_selected = selector.fit_transform(X_train_resampled, y_train_resampled)\n",
    "selected_features = X_train_resampled.columns[selector.get_support()]   #Select a list of column names for the characteristics.\n",
    "print(\"Selected Features:\")\n",
    "print(selected_features)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train_resampled_selected, y_train_resampled)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# Get the best model\n",
    "best_rf_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the validation set with selected features\n",
    "X_val_selected = selector.transform(X_val)\n",
    "val_predictions = best_rf_classifier.predict(X_val_selected)\n",
    "# Calculate accuracy on validation set\n",
    "val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "print(\"Validation Accuracy with Best Model:\", val_accuracy)\n",
    "\n",
    "\n",
    "# Make predictions on the test set with the best model\n",
    "X_test_selected = selector.transform(X_test)\n",
    "test_predictions = best_rf_classifier.predict(X_test_selected)\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(\"Test Accuracy with Best Model:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38929a15-8dea-4b96-a6e9-00dac2aa4eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7325826882477129\n",
      "Precision: 0.20045045045045046\n",
      "Recall: 0.7807017543859649\n",
      "F1 Score: 0.31899641577060933\n",
      "Micro F1 Score: 0.7325826882477129\n",
      "Macro F1 Score: 0.5763108173423975\n",
      "ROC AUC: 0.847289896508678\n",
      "Confusion Matrix:\n",
      "[[952 355]\n",
      " [ 25  89]]\n"
     ]
    }
   ],
   "source": [
    "#The evaluation metrics on the test set are calculated directly without threshold selection and re-prediction.\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, precision_recall_curve, roc_curve, confusion_matrix\n",
    "\n",
    "#Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test, test_predictions)\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_test, test_predictions)\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_test, test_predictions)\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "# Calculate micro F1 score\n",
    "micro_f1 = f1_score(y_test, test_predictions, average='micro')\n",
    "print(\"Micro F1 Score:\", micro_f1)\n",
    "\n",
    "# Calculate macro F1 score\n",
    "macro_f1 = f1_score(y_test, test_predictions, average='macro')\n",
    "print(\"Macro F1 Score:\", macro_f1)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "y_prob = best_rf_classifier.predict_proba(X_test_selected)[:, 1]\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9228af36-d91a-44b9-ba3c-a77f93a903ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80fd8821-7338-45dc-a3e0-949fefa793bc",
   "metadata": {},
   "source": [
    "### 2. XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca37366c-2d39-4c57-9288-5724a2d0fe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.1 SMOTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96464bb3-c663-4853-bdf8-b2a44daa0974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    3919\n",
      "1    3919\n",
      "Name: status_label_encoded, dtype: int64\n",
      "[0]\ttrain-logloss:0.63582\teval-logloss:0.64850\n",
      "[1]\ttrain-logloss:0.58642\teval-logloss:0.61046\n",
      "[2]\ttrain-logloss:0.54256\teval-logloss:0.57689\n",
      "[3]\ttrain-logloss:0.50294\teval-logloss:0.54653\n",
      "[4]\ttrain-logloss:0.46687\teval-logloss:0.51802\n",
      "[5]\ttrain-logloss:0.43478\teval-logloss:0.49362\n",
      "[6]\ttrain-logloss:0.40536\teval-logloss:0.47202\n",
      "[7]\ttrain-logloss:0.37844\teval-logloss:0.45221\n",
      "[8]\ttrain-logloss:0.35386\teval-logloss:0.43448\n",
      "[9]\ttrain-logloss:0.33148\teval-logloss:0.41826\n",
      "[10]\ttrain-logloss:0.31078\teval-logloss:0.40376\n",
      "[11]\ttrain-logloss:0.29201\teval-logloss:0.39025\n",
      "[12]\ttrain-logloss:0.27442\teval-logloss:0.37828\n",
      "[13]\ttrain-logloss:0.25815\teval-logloss:0.36630\n",
      "[14]\ttrain-logloss:0.24311\teval-logloss:0.35527\n",
      "[15]\ttrain-logloss:0.22933\teval-logloss:0.34650\n",
      "[16]\ttrain-logloss:0.21677\teval-logloss:0.33846\n",
      "[17]\ttrain-logloss:0.20485\teval-logloss:0.33027\n",
      "[18]\ttrain-logloss:0.19347\teval-logloss:0.32245\n",
      "[19]\ttrain-logloss:0.18304\teval-logloss:0.31548\n",
      "[20]\ttrain-logloss:0.17336\teval-logloss:0.30845\n",
      "[21]\ttrain-logloss:0.16415\teval-logloss:0.30240\n",
      "[22]\ttrain-logloss:0.15564\teval-logloss:0.29663\n",
      "[23]\ttrain-logloss:0.14781\teval-logloss:0.29145\n",
      "[24]\ttrain-logloss:0.14027\teval-logloss:0.28694\n",
      "[25]\ttrain-logloss:0.13338\teval-logloss:0.28200\n",
      "[26]\ttrain-logloss:0.12694\teval-logloss:0.27823\n",
      "[27]\ttrain-logloss:0.12084\teval-logloss:0.27486\n",
      "[28]\ttrain-logloss:0.11524\teval-logloss:0.27152\n",
      "[29]\ttrain-logloss:0.10988\teval-logloss:0.26849\n",
      "[30]\ttrain-logloss:0.10492\teval-logloss:0.26602\n",
      "[31]\ttrain-logloss:0.10021\teval-logloss:0.26287\n",
      "[32]\ttrain-logloss:0.09581\teval-logloss:0.26088\n",
      "[33]\ttrain-logloss:0.09147\teval-logloss:0.25818\n",
      "[34]\ttrain-logloss:0.08762\teval-logloss:0.25613\n",
      "[35]\ttrain-logloss:0.08395\teval-logloss:0.25444\n",
      "[36]\ttrain-logloss:0.08045\teval-logloss:0.25272\n",
      "[37]\ttrain-logloss:0.07717\teval-logloss:0.25073\n",
      "[38]\ttrain-logloss:0.07398\teval-logloss:0.24907\n",
      "[39]\ttrain-logloss:0.07102\teval-logloss:0.24766\n",
      "[40]\ttrain-logloss:0.06828\teval-logloss:0.24649\n",
      "[41]\ttrain-logloss:0.06569\teval-logloss:0.24545\n",
      "[42]\ttrain-logloss:0.06318\teval-logloss:0.24350\n",
      "[43]\ttrain-logloss:0.06076\teval-logloss:0.24244\n",
      "[44]\ttrain-logloss:0.05847\teval-logloss:0.24132\n",
      "[45]\ttrain-logloss:0.05631\teval-logloss:0.24048\n",
      "[46]\ttrain-logloss:0.05423\teval-logloss:0.23935\n",
      "[47]\ttrain-logloss:0.05235\teval-logloss:0.23904\n",
      "[48]\ttrain-logloss:0.05049\teval-logloss:0.23808\n",
      "[49]\ttrain-logloss:0.04873\teval-logloss:0.23765\n",
      "[50]\ttrain-logloss:0.04700\teval-logloss:0.23704\n",
      "[51]\ttrain-logloss:0.04541\teval-logloss:0.23656\n",
      "[52]\ttrain-logloss:0.04389\teval-logloss:0.23577\n",
      "[53]\ttrain-logloss:0.04248\teval-logloss:0.23493\n",
      "[54]\ttrain-logloss:0.04109\teval-logloss:0.23482\n",
      "[55]\ttrain-logloss:0.03983\teval-logloss:0.23454\n",
      "[56]\ttrain-logloss:0.03858\teval-logloss:0.23394\n",
      "[57]\ttrain-logloss:0.03741\teval-logloss:0.23337\n",
      "[58]\ttrain-logloss:0.03632\teval-logloss:0.23323\n",
      "[59]\ttrain-logloss:0.03526\teval-logloss:0.23254\n",
      "[60]\ttrain-logloss:0.03424\teval-logloss:0.23211\n",
      "[61]\ttrain-logloss:0.03322\teval-logloss:0.23126\n",
      "[62]\ttrain-logloss:0.03230\teval-logloss:0.23076\n",
      "[63]\ttrain-logloss:0.03140\teval-logloss:0.23073\n",
      "[64]\ttrain-logloss:0.03053\teval-logloss:0.23060\n",
      "[65]\ttrain-logloss:0.02970\teval-logloss:0.23065\n",
      "[66]\ttrain-logloss:0.02890\teval-logloss:0.23045\n",
      "[67]\ttrain-logloss:0.02811\teval-logloss:0.22995\n",
      "[68]\ttrain-logloss:0.02735\teval-logloss:0.22912\n",
      "[69]\ttrain-logloss:0.02666\teval-logloss:0.22915\n",
      "[70]\ttrain-logloss:0.02597\teval-logloss:0.22907\n",
      "[71]\ttrain-logloss:0.02532\teval-logloss:0.22916\n",
      "[72]\ttrain-logloss:0.02474\teval-logloss:0.22943\n",
      "[73]\ttrain-logloss:0.02413\teval-logloss:0.22887\n",
      "[74]\ttrain-logloss:0.02353\teval-logloss:0.22895\n",
      "[75]\ttrain-logloss:0.02302\teval-logloss:0.22941\n",
      "[76]\ttrain-logloss:0.02245\teval-logloss:0.22833\n",
      "[77]\ttrain-logloss:0.02195\teval-logloss:0.22849\n",
      "[78]\ttrain-logloss:0.02145\teval-logloss:0.22878\n",
      "[79]\ttrain-logloss:0.02097\teval-logloss:0.22890\n",
      "[80]\ttrain-logloss:0.02050\teval-logloss:0.22926\n",
      "[81]\ttrain-logloss:0.02006\teval-logloss:0.22952\n",
      "[82]\ttrain-logloss:0.01965\teval-logloss:0.22976\n",
      "[83]\ttrain-logloss:0.01917\teval-logloss:0.22970\n",
      "[84]\ttrain-logloss:0.01879\teval-logloss:0.22966\n",
      "[85]\ttrain-logloss:0.01839\teval-logloss:0.22979\n",
      "Test Accuracy: 0.9261083743842364\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "\n",
    "# Split the dataset into features and labels\n",
    "X = data_cleaned.drop('status_label_encoded', axis=1)\n",
    "y = data_cleaned['status_label_encoded']\n",
    "\n",
    "# Split the data into training + validation and testing sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Further split the training + validation set into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)\n",
    "\n",
    "# Apply SMOTE only on the training set\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "# Print the number of samples after oversampling\n",
    "print(pd.Series(y_train_resampled).value_counts())\n",
    "\n",
    "# Convert data to DMatrix format for XGBoost\n",
    "dtrain = xgb.DMatrix(X_train_resampled, label=y_train_resampled)\n",
    "dval = xgb.DMatrix(X_val, label=y_val)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary:logistic',  # Binary classification\n",
    "    'eval_metric': 'logloss',         # Logarithmic loss\n",
    "    'eta': 0.07,                       # Learning rate\n",
    "    'max_depth': 60,                   # Maximum depth of the tree\n",
    "    'subsample': 0.9,                 # Subsample ratio of the training instances\n",
    "    'colsample_bytree': 0.9,          # Subsample ratio of columns when constructing each tree\n",
    "    'lambda': 1,                      # L2 regularization term (default is 1)\n",
    "    'alpha': 0,                       # L1 regularization term (default is 0)\n",
    "    #'num_rounds': 100,                # Number of boosting rounds\n",
    "    'seed': 42                        # Random seed\n",
    "}\n",
    "\n",
    "# Train XGBoost model\n",
    "num_rounds = 100\n",
    "watchlist = [(dtrain, 'train'), (dval, 'eval')]\n",
    "xgb_model = xgb.train(params, dtrain, num_rounds, evals=watchlist, early_stopping_rounds=10)\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions_proba = xgb_model.predict(dtest)\n",
    "test_predictions = [1 if x > 0.5 else 0 for x in test_predictions_proba]\n",
    "\n",
    "# Calculate accuracy on test set\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8edcafa2-733b-405e-901d-f1b596e3e118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9261083743842364\n",
      "Precision: 0.5818181818181818\n",
      "Recall: 0.2807017543859649\n",
      "F1 Score: 0.378698224852071\n",
      "Micro F1 Score: 0.9261083743842364\n",
      "Macro F1 Score: 0.6697082594518492\n",
      "ROC AUC: 0.8486556866535121\n",
      "Confusion Matrix:\n",
      "[[1284   23]\n",
      " [  82   32]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test, test_predictions)\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_test, test_predictions)\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_test, test_predictions)\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "# Calculate micro F1 score\n",
    "micro_f1 = f1_score(y_test, test_predictions, average='micro')\n",
    "print(\"Micro F1 Score:\", micro_f1)\n",
    "\n",
    "# Calculate macro F1 score\n",
    "macro_f1 = f1_score(y_test, test_predictions, average='macro')\n",
    "print(\"Macro F1 Score:\", macro_f1)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, test_predictions_proba)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "775f5465-3caa-4d00-908a-bdff4eecaab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 SMOTE + FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b2d1469-e2c3-4039-b7cb-448d7139d0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 SMOTE + FS + GRID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ece5d910-f089-45fd-aa80-8e42381984d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features:\n",
      "Index(['X4_last1year', 'X4_last2year', 'X9_last1year', 'X9_last2year',\n",
      "       'X11_last1year', 'X11_last2year', 'X16_last2year', 'X18_last2year',\n",
      "       'X4_last2year_ycr', 'X10_last1year_ycr', 'X15_last1year_ycr',\n",
      "       'nyse_last1year', 'nyse_last2year', 'nasdaq_last2year',\n",
      "       'Division_encoded'],\n",
      "      dtype='object')\n",
      "Best Parameters: {'colsample_bytree': 0.9, 'learning_rate': 0.1, 'max_depth': 70, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
      "Best Score: 0.9584086809580246\n",
      "Val Accuracy: 0.9126760563380282\n",
      "Test Accuracy: 0.9148486980999296\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Split the dataset into features and labels\n",
    "X = data_cleaned.drop('status_label_encoded', axis=1)\n",
    "y = data_cleaned['status_label_encoded']\n",
    "\n",
    "# Split the data into training + validation and testing sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split the training + validation set into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)\n",
    "\n",
    "# Apply SMOTE only on the training set\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define a XGBoost classifier\n",
    "xgb_classifier = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "# Perform feature selection \n",
    "selector = SelectFromModel(estimator=xgb_classifier, threshold='mean')  # threshold: mean\n",
    "selector.fit(X_train_resampled, y_train_resampled)\n",
    "selected_features = X_train_resampled.columns[selector.get_support()]\n",
    "print(\"Selected Features:\")\n",
    "print(selected_features)\n",
    "\n",
    "# Transform the datasets\n",
    "X_train_selected = selector.transform(X_train_resampled)\n",
    "X_val_selected = selector.transform(X_val)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'max_depth': [5, 25, 70],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0], \n",
    "}\n",
    "\n",
    "# Create cross-validation folds\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Create GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=xgb_classifier, param_grid=param_grid, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Fit GridSearchCV to data\n",
    "grid_search.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Print best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# Train XGBoost model with best parameters\n",
    "best_params = grid_search.best_params_\n",
    "best_xgb_classifier = grid_search.best_estimator_\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions on the validation set\n",
    "val_predictions = best_xgb_classifier.predict(X_val_selected)\n",
    "\n",
    "# Calculate accuracy on test set\n",
    "val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "print(\"Val Accuracy:\", val_accuracy)\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = best_xgb_classifier.predict(X_test_selected)\n",
    "\n",
    "# Calculate accuracy on test set\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18809830-0339-4e21-b287-05dfccb9671d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9148486980999296\n",
      "Precision: 0.45977011494252873\n",
      "Recall: 0.3508771929824561\n",
      "F1 Score: 0.39800995024875624\n",
      "Micro F1 Score: 0.9148486980999296\n",
      "Macro F1 Score: 0.6760969857264228\n",
      "ROC AUC: 0.8511926334581671\n",
      "Confusion Matrix:\n",
      "[[1260   47]\n",
      " [  74   40]]\n"
     ]
    }
   ],
   "source": [
    "#The evaluation metrics on the test set are calculated directly without threshold selection and re-prediction.\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, precision_recall_curve, roc_curve, confusion_matrix\n",
    "\n",
    "#Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test, test_predictions)\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_test, test_predictions)\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_test, test_predictions)\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "# Calculate micro F1 score\n",
    "micro_f1 = f1_score(y_test, test_predictions, average='micro')\n",
    "print(\"Micro F1 Score:\", micro_f1)\n",
    "\n",
    "# Calculate macro F1 score\n",
    "macro_f1 = f1_score(y_test, test_predictions, average='macro')\n",
    "print(\"Macro F1 Score:\", macro_f1)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "y_prob = best_xgb_classifier.predict_proba(X_test_selected)[:, 1]\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9289b3b2-5aec-457f-ac51-538933822c65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#2.4 UNDERSAMPLING + FS +GRID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b185805-44a9-42d4-83ed-29607e53d89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features:\n",
      "Index(['X1_last2year', 'X2_last1year', 'X2_last2year', 'X3_last2year',\n",
      "       'X4_last2year', 'X5_last1year', 'X5_last2year', 'X8_last1year',\n",
      "       'X10_last2year', 'X11_last1year', 'X11_last2year', 'X12_last1year',\n",
      "       'X12_last2year', 'X14_last2year', 'X15_last1year', 'X15_last2year',\n",
      "       'X16_last1year', 'X18_last2year', 'X1_last1year_ycr',\n",
      "       'X3_last2year_ycr', 'X5_last2year_ycr', 'X7_last2year_ycr',\n",
      "       'X8_last1year_ycr', 'X9_last2year_ycr', 'X11_last2year_ycr',\n",
      "       'X14_last2year_ycr', 'X15_last1year_ycr', 'X18_last1year_ycr',\n",
      "       'nyse_last1year', 'nasdaq_last1year', 'nasdaq_last2year',\n",
      "       'MajorGroup_encoded'],\n",
      "      dtype='object')\n",
      "Best Parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 50, 'subsample': 0.9}\n",
      "Best Score: 0.7830721339630743\n",
      "Val Accuracy: 0.7323943661971831\n",
      "Test Accuracy: 0.7361013370865588\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Split the dataset into features and labels\n",
    "X = data_cleaned.drop('status_label_encoded', axis=1)\n",
    "y = data_cleaned['status_label_encoded']\n",
    "\n",
    "# Split the data into training + validation and testing sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Further split the training + validation set into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)\n",
    "\n",
    "# Define an undersampler\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = undersampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define a XGBoost classifier\n",
    "xgb_classifier = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "# Perform feature selection \n",
    "selector = SelectFromModel(estimator=xgb_classifier, threshold='mean')  # threshold: mean\n",
    "selector.fit(X_train_resampled, y_train_resampled)\n",
    "selected_features = X_train_resampled.columns[selector.get_support()]\n",
    "print(\"Selected Features:\")\n",
    "print(selected_features)\n",
    "\n",
    "# Transform the datasets\n",
    "X_train_selected = selector.transform(X_train_resampled)\n",
    "X_val_selected = selector.transform(X_val)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'max_depth': [5, 25, 70],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0],     \n",
    "}\n",
    "\n",
    "# Create cross-validation folds\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Create GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=xgb_classifier, param_grid=param_grid, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Fit GridSearchCV to data\n",
    "grid_search.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Print best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# Train XGBoost model with best parameters\n",
    "best_params = grid_search.best_params_\n",
    "best_xgb_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the validation set\n",
    "val_predictions = best_xgb_classifier.predict(X_val_selected)\n",
    "\n",
    "# Calculate accuracy on test set\n",
    "val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "print(\"Val Accuracy:\", val_accuracy)\n",
    "\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = best_xgb_classifier.predict(X_test_selected)\n",
    "\n",
    "# Calculate accuracy on test set\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5124c231-eaf4-474b-80c9-c3caaa427438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7361013370865588\n",
      "Precision: 0.2054176072234763\n",
      "Recall: 0.7982456140350878\n",
      "F1 Score: 0.3267504488330341\n",
      "Micro F1 Score: 0.7361013370865588\n",
      "Macro F1 Score: 0.5813183316375237\n",
      "ROC AUC: 0.8414609592075061\n",
      "Confusion Matrix:\n",
      "[[955 352]\n",
      " [ 23  91]]\n"
     ]
    }
   ],
   "source": [
    "#The evaluation metrics on the test set are calculated directly without threshold selection and re-prediction.\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, precision_recall_curve, roc_curve, confusion_matrix\n",
    "\n",
    "#Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test, test_predictions)\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_test, test_predictions)\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_test, test_predictions)\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "# Calculate micro F1 score\n",
    "micro_f1 = f1_score(y_test, test_predictions, average='micro')\n",
    "print(\"Micro F1 Score:\", micro_f1)\n",
    "\n",
    "# Calculate macro F1 score\n",
    "macro_f1 = f1_score(y_test, test_predictions, average='macro')\n",
    "print(\"Macro F1 Score:\", macro_f1)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "y_prob = best_xgb_classifier.predict_proba(X_test_selected)[:, 1]\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f705495-0cf3-4d0c-891d-ccd5586d0ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
